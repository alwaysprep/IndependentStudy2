collector_agent.sources = tester1 tester2 tester3
collector_agent.channels = stream store
collector_agent.sinks = spark hdfs


collector_agent.sources.tester1.type = spooldir
collector_agent.sources.tester1.channels = stream store
collector_agent.sources.tester1.spoolDir = /Users/doktoray/workspace/testo/Datasets/testers/tester1/input
collector_agent.sources.tester1.fileHeader = true

collector_agent.sources.tester2.type = spooldir
collector_agent.sources.tester2.channels = stream store
collector_agent.sources.tester2.spoolDir = /Users/doktoray/workspace/testo/Datasets/testers/tester2/input
collector_agent.sources.tester2.fileHeader = true

collector_agent.sources.tester3.type = spooldir
collector_agent.sources.tester3.channels = stream store
collector_agent.sources.tester3.spoolDir = /Users/doktoray/workspace/testo/Datasets/testers/tester3/input
collector_agent.sources.tester3.fileHeader = true


collector_agent.channels.stream.type = memory
collector_agent.channels.store.type = memory


collector_agent.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
collector_agent.sinks.spark.hostname = localhost
collector_agent.sinks.spark.port = 9999
collector_agent.sinks.spark.channel = stream

collector_agent.sinks.hdfs.type = file_roll
collector_agent.sinks.hdfs.channel = store
collector_agent.sinks.hdfs.sink.directory = /Users/doktoray/workspace/testo/Datasets/hdfs/raw
collector_agent.sinks.hdfs.sink.pathManager.extension = txt
#never roll-based on time
collector_agent.sinks.hdfs.sink.rollInterval = 0
#300MB = 1024 * 1024 * 300
collector_agent.sinks.hdfs.sink.rollSize = 314572800
#never roll base on number of events
collector_agent.sinks.hdfs.sink.rollCount = 0



